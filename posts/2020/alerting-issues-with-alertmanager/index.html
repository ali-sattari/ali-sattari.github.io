<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Alerting issues with Alertmanager - Ali's Blog</title><meta name=viewport content="width=device-width,initial-scale=1"><meta itemprop=name content="Alerting issues with Alertmanager"><meta itemprop=description content="Alerting is a vital part of operations, as it frees humans from watching systems and let them tend to more creative and constructive activities, until something breaks&mldr; at 3 am, but then resolves in few minutes, then fires again 10 minutes later, and that&rsquo;s when you know alerting isn&rsquo;t working properly!"><meta itemprop=datePublished content="2020-08-13T09:00:00+02:00"><meta itemprop=dateModified content="2020-08-13T09:00:00+02:00"><meta itemprop=wordCount content="1243"><meta itemprop=keywords content="prometheus,alertmanager,alerting,"><meta property="og:title" content="Alerting issues with Alertmanager"><meta property="og:description" content="Alerting is a vital part of operations, as it frees humans from watching systems and let them tend to more creative and constructive activities, until something breaks&mldr; at 3 am, but then resolves in few minutes, then fires again 10 minutes later, and that&rsquo;s when you know alerting isn&rsquo;t working properly!"><meta property="og:type" content="article"><meta property="og:url" content="https://ali.sattari.me/posts/2020/alerting-issues-with-alertmanager/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-08-13T09:00:00+02:00"><meta property="article:modified_time" content="2020-08-13T09:00:00+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Alerting issues with Alertmanager"><meta name=twitter:description content="Alerting is a vital part of operations, as it frees humans from watching systems and let them tend to more creative and constructive activities, until something breaks&mldr; at 3 am, but then resolves in few minutes, then fires again 10 minutes later, and that&rsquo;s when you know alerting isn&rsquo;t working properly!"><link href='https://fonts.googleapis.com/css?family=Playfair+Display:700' rel=stylesheet type=text/css><link rel=stylesheet type=text/css media=screen href=https://ali.sattari.me/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://ali.sattari.me/css/main.css><link rel=stylesheet type=text/css href=https://ali.sattari.me/css/custom.css><link id=dark-scheme rel=stylesheet type=text/css href=https://ali.sattari.me/css/dark.css><script src=https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js></script>
<script src=https://ali.sattari.me/js/main.js></script></head><body><div class="container wrapper"><div class=header><h1 class=site-title><a href=https://ali.sattari.me/>Ali's Blog</a></h1><div class=site-description><p>Musings about technology, books and skepticism!</p><nav class="nav social"><ul class=flat><li><a href=https://github.com/ali-sattari title=Github><i data-feather=github></i></a></li><li><a href=https://twitter.com/ali_sattari title=Twitter><i data-feather=twitter></i></a></li><li><a href=https://www.goodreads.com/user/show/4719513-ali-sattari title=Goodreads><i data-feather=book-open></i></a></li><li><a href=https://www.linkedin.com/in/alisattari/ title=LinkedIn><i data-feather=linkedin></i></a></li><li><a href=/index.xml title=RSS><i data-feather=rss></i></a></li></ul></nav><span class=scheme-toggle><a href=# id=scheme-toggle></a></div><nav class=nav><ul class=flat><li><a href=/about>About</a></li><li><a href=/posts>All posts</a></li><li><a href=/tags>Tags</a></li></ul></nav></div><div class=post><div class=post-header><div class=meta><div class=date><span class=day>13</span>
<span class=rest>Aug 2020</span></div></div><div class=matter><h1 class=title>Alerting issues with Alertmanager</h1></div></div><div class=markdown><p>Alerting is a vital part of operations, as it frees humans from watching systems and let them tend to more creative and constructive activities, until something breaks&mldr; at 3 am, but then resolves in few minutes, then fires again 10 minutes later, and that&rsquo;s when you know alerting isn&rsquo;t working properly!</p><p>You are running your monitoring stack with Prometheus and Alertmanager, and since you are someone with proper reliability in mind, you have a cluster of at least three Alertmanagers, perfect! Perhaps this works for a while with no issues, but entropy and chaos are always on the hunt. It is not that no alerts come through or all instances are down, it works but with some occasional annoyances: alerts flapping or receiving duplicate notification of a state. As you might have experienced, blackouts are relatively easy to fix, brownouts, on the other hand, are often elusive and hard to resolve.</p><p>Let&rsquo;s define the issues. flapping happens when an alert goes into firing and resolved state repeatedly in a short window of time, like triggering again right after it was resolved a few minutes ago. This can be misleading, you want to rely on the alert state to some extent to triage issues or to decide whether you should get out of the bed at 3 am, and when alerts flap, that trust is quickly eroded. Duplicate notifications are notifications delivered more than once while the situation that triggered the alert is still ongoing. They add to the noise and chaos, you might have a Slack channel for your alerts and prefer to have a clear view of last notifications in that stream, duplicates mess that up. So in a sense flapping is the more serious of the two, but both are annoying.</p><h2 id=quick-primer-on-how-alerting-works-in-prometheus>Quick primer on how alerting works in Prometheus</h2><p>Like a good troubleshooter that you are, you would want to find the source of the problem! But to get to the source, you need to know the path.</p><ul><li>Prometheus has the data (metrics and meta-metrics<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>) and evaluates rules that are defined in the <a href=https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/>configuration</a>.</li><li>Both scraping and rule evaluation happens at regular, pre-defined intervals<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</li><li>Prometheus evaluates the alert rule and if the <code>expr</code> returns <code>true</code><sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> the alert goes into <code>pending</code> state for the duration defined by <code>for</code>, this is to help weed out the transient issues.</li><li>When the <code>for</code> duration is passed and <code>expr</code> still evals to <code>true</code>, the alert goes into <code>firing</code> state and is now sent to Alertmanager. This happens on all consequent eval intervals from this point until the <code>expr</code> no longer evals to <code>true</code>.</li><li>Meanwhile, there are a couple of intervals and waits configured in Alertmsnager as well, such as <code>group_interval</code> and <code>repeat_interval</code>. Alertmanager waits for <code>group_interval</code> to see if any further alerts come in with the same <code>group_by</code> criteria so they can be merged, and finally, a notification is generated, goes through <a href=https://prometheus.io/docs/alerting/latest/configuration/#route>routing</a> and is sent to the intended <a href=https://prometheus.io/docs/alerting/latest/configuration/#receiver>receivers</a>.</li><li>Remember that Prometheus continues to fire alerts toward Alertmanager on <code>eval_interval</code> while the issue persists and after the initial notification, Alertmanager sends notification only on <code>repeat_interval</code> (so that a firing alert isn&rsquo;t forgotten) or when it is resolved. But when is an alert resolved in Alertmanager? There is a <code>resolve_timeout</code> in the configuration, if no alert is received by Prometheus in that duration the alert is considered resolved. Prometheus can also send resolved state (by setting the EndsAt timestamp).</li></ul><p>How can you observe this process? good news is there is <code>ALERTS</code> metric exposed by Prometheus that records the state of each alert. So you can see when it went into pending, firing, and stopped firing (resolved). The bad news is this is the only thing! there are no alert specific metrics exposed by Alertmanager, there are log entries for some actions, but they often don&rsquo;t match 1:1 to the flow described above.</p><h2 id=how-to-find-the-source>How to find the source?</h2><p>Now that you know the flow, it is easier to look for the source of the problem. First thing is to check the alert <code>expr</code> and <code>ALERTS{alertname="YOUR_SHINY_ALERT"}</code> when flapping happens, if both are continuous graphs with no change of state, you know the alert rule definition is fine. If however, you see a lot of pending -> firing in that window, you should look more closely at your alert definition. It can be the case that <code>for</code> is missing, which means no <code>pending</code> state, alert fires with the first eval to true. Or maybe <code>for</code> is too short<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> or finally, the expression is at fault and fluctuates below and above a static threshold.</p><p>A subtle but important point is that the way Alertmanager cluster works, it expects Prometheus<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> to send firing alerts to all Alertmanager instances<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>. If however, Prometheus is only sending alerts to one instance (e.g they are behind an ingress or loadbalancer), the cluster doesn&rsquo;t get to do its job and each instance develops a brain of its own to some extent. So while one Alertmanager has recently received an alert from Prometheus, another one already thinks it is resolved (through <code>resolve_timout</code>) and sends out a resolved notification, and then receives the alert again and considers it a new alert and so on. Make sure Prometheus <a href=https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alertmanager_config>points</a> to all instances of the Alertmanager by static config or proper service discovery.</p><p>Alertmanager in <a href=https://github.com/prometheus/alertmanager#high-availability>cluster mode</a> uses a separate port to connect instances to identify peers and communicate events. If <code>--cluster.*</code> parameters aren&rsquo;t properly set, a quorum can&rsquo;t form, and deduplication and state sync won&rsquo;t happen. This can be observed both in logs and in metrics exposed by Alertmanager:</p><ul><li><code>alertmanager_cluster_members</code> records number of peers each instance can see, this should be equal among instances, meaning they can all see each other, so for a cluster of 3 Alertmanagers this should be 3 all the time, except briefly for restarts or reschedulings of instances.</li><li><code>alertmanager_peer_position</code> shows where each instance stands, this should also be a fixed number per instance through time except when one is rescheduled and the positions might reshuffle.</li></ul><h2 id=useful-alerts-on-alerting>Useful alerts on alerting!</h2><p>Let&rsquo;s say you have found and fixed the issues, now all run perfectly and faith in alerting is restored, a great accomplishment. But then there is always entropy! This can happen again: a version upgrade, change in CI/CD pipelines, IaC refactoring, adding features, or new integrations to the system? who knows, someday someone or something might inadvertently ruin this beautiful setup. How to guard against it? changing the profession is always an option, but luckily there is also alerting! so meta! isn&rsquo;t it?</p><h3 id=changes-in-cluster-members>Changes in cluster members</h3><p>If this holds true for ~10m you have a problem in your Alertmanager cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-SQL data-lang=SQL><span style=display:flex><span><span style=color:#00f>avg</span>(alertmanager_cluster_members) <span style=color:#00f>by</span> (your_cluster)
</span></span><span style=display:flex><span>!=
</span></span><span style=display:flex><span><span style=color:#00f>max</span>(alertmanager_cluster_members) <span style=color:#00f>by</span> (your_cluster)
</span></span></code></pre></div><h3 id=alerts-not-being-sent-to-all-instances>Alerts not being sent to all instances</h3><p>This marks deviations more than 5% in the number of alerts received by each instance compared to the cluster average, if this persists more than ~10m, alerts are not being sent to all Alertmanager instances.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-SQL data-lang=SQL><span style=display:flex><span><span style=color:#00f>abs</span>(
</span></span><span style=display:flex><span>  rate(alertmanager_alerts_received_total<span>{</span>status=<span style=color:#a31515>&#34;firing&#34;</span><span>}</span>[10m])
</span></span><span style=display:flex><span>  - <span style=color:#00f>on</span> (your_cluster) group_left()
</span></span><span style=display:flex><span>  <span style=color:#00f>avg</span>(rate(alertmanager_alerts_received_total<span>{</span>status=<span style=color:#a31515>&#34;firing&#34;</span><span>}</span>[10m])) <span style=color:#00f>by</span> (your_cluster)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>/
</span></span><span style=display:flex><span>rate(alertmanager_alerts_received_total<span>{</span>status=<span style=color:#a31515>&#34;firing&#34;</span><span>}</span>[10m])
</span></span><span style=display:flex><span>* 100 &gt; 5
</span></span></code></pre></div><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Such as <code>up</code> which provides data about the scrape target in a pull model. <a href=https://prometheus.io/docs/concepts/jobs_instances/#automatically-generated-labels-and-time-series>ref</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>As <code>evaluation_interval</code> in main Prometheus <a href=https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration-file>config</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>As result of a binary comparison or as a non-empty result (a vector)&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>My personal rule of thumb is to have it 3-4x of eval interval&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Basically any alert client for that matter, e.g Thanos Rule.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>Surprisingly this can&rsquo;t be found in Prometheus or Alertmanager documentation, but is mentioned in <a href=https://www.robustperception.io/high-availability-prometheus-alerting-and-notification>this blog post</a> and also in <a href=https://coreos.com/operators/prometheus/docs/latest/high-availability.html#alertmanager>CoreOS docs</a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=tags><ul class=flat><li><a href=/tags/prometheus>prometheus</a></li><li><a href=/tags/alertmanager>alertmanager</a></li><li><a href=/tags/alerting>alerting</a></li></ul></div></div></div><div class="footer wrapper"><nav class=nav><div>2020 All contents under <a href=https://www.gnu.org/licenses/fdl-1.3.html>GNU FDL</a> | <a href=https://github.com/knadh/hugo-ink>Ink</a> theme on <a href=https://gohugo.io>Hugo</a></div></nav></div><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-101616403-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<script>feather.replace()</script></body></html>