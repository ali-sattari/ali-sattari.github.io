<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Prometheus at Scale with Thanos: Timing Parameters - Ali's Blog</title><meta name=viewport content="width=device-width,initial-scale=1"><meta itemprop=name content="Prometheus at Scale with Thanos: Timing Parameters"><meta itemprop=description content="The story begins a few weeks or months after you have had the epiphany to scale out your Prometheus setup with Thanos. The new setup runs smoothly for a while with default parameters and you can shovel more resources from time to time to improve the performance. Such beautiful and simple times will soon be gone; one day you wake up to the reality that some heavy queries are taking most of the resources and grinding the whole system to a halt under load."><meta itemprop=datePublished content="2021-02-01T09:00:00+01:00"><meta itemprop=dateModified content="2021-02-01T09:00:00+01:00"><meta itemprop=wordCount content="1029"><meta itemprop=keywords content="prometheus,thanos,monitoring,"><meta property="og:title" content="Prometheus at Scale with Thanos: Timing Parameters"><meta property="og:description" content="The story begins a few weeks or months after you have had the epiphany to scale out your Prometheus setup with Thanos. The new setup runs smoothly for a while with default parameters and you can shovel more resources from time to time to improve the performance. Such beautiful and simple times will soon be gone; one day you wake up to the reality that some heavy queries are taking most of the resources and grinding the whole system to a halt under load."><meta property="og:type" content="article"><meta property="og:url" content="https://ali.sattari.me/posts/2021/prometheus-at-scale-with-thanos-timing-parameters/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-02-01T09:00:00+01:00"><meta property="article:modified_time" content="2021-02-01T09:00:00+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Prometheus at Scale with Thanos: Timing Parameters"><meta name=twitter:description content="The story begins a few weeks or months after you have had the epiphany to scale out your Prometheus setup with Thanos. The new setup runs smoothly for a while with default parameters and you can shovel more resources from time to time to improve the performance. Such beautiful and simple times will soon be gone; one day you wake up to the reality that some heavy queries are taking most of the resources and grinding the whole system to a halt under load."><link href='https://fonts.googleapis.com/css?family=Playfair+Display:700' rel=stylesheet type=text/css><link rel=stylesheet type=text/css media=screen href=https://ali.sattari.me/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://ali.sattari.me/css/main.css><link rel=stylesheet type=text/css href=https://ali.sattari.me/css/custom.css><link id=dark-scheme rel=stylesheet type=text/css href=https://ali.sattari.me/css/dark.css><script src=https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js></script>
<script src=https://ali.sattari.me/js/main.js></script></head><body><div class="container wrapper"><div class=header><h1 class=site-title><a href=https://ali.sattari.me/>Ali's Blog</a></h1><div class=site-description><p>Musings about technology, books and skepticism!</p><nav class="nav social"><ul class=flat><li><a href=https://github.com/ali-sattari title=Github><i data-feather=github></i></a></li><li><a href=https://twitter.com/ali_sattari title=Twitter><i data-feather=twitter></i></a></li><li><a href=https://hardcover.app/@ali.sattari title=Hardcover><i data-feather=book-open></i></a></li><li><a href=https://www.linkedin.com/in/alisattari/ title=LinkedIn><i data-feather=linkedin></i></a></li><li><a href=/index.xml title=RSS><i data-feather=rss></i></a></li></ul></nav><span class=scheme-toggle><a href=# id=scheme-toggle></a></div><nav class=nav><ul class=flat><li><a href=/about>About</a></li><li><a href=/posts>All posts</a></li><li><a href=/tags>Tags</a></li></ul></nav></div><div class=post><div class=post-header><div class=meta><div class=date><span class=day>01</span>
<span class=rest>Feb 2021</span></div></div><div class=matter><h1 class=title>Prometheus at Scale with Thanos: Timing Parameters</h1></div></div><div class=markdown><p>The story begins a few weeks or months after you have had the epiphany to scale out your Prometheus setup with Thanos. The new setup runs smoothly for a while with default parameters and you can shovel more resources from time to time to improve the performance. Such beautiful and simple times will soon be gone; one day you wake up to the reality that some heavy queries are taking most of the resources and grinding the whole system to a halt under load. The way these issues manifest is often through slowness and unresponsiveness of the whole system, which eventually can recover on it&rsquo;s own but take long time.</p><p>But why this happen? two things are inevitable: users and chaos. Users of your system might hate you, themselves or the whole system and put a heavy query in a Grafana dashboard with <code>5s</code> refresh interval just for the heck of it. Most cloud computing environments nowadays are well-known agents of chaos, Your applications can and will get rescheduled for variety of reasons, especially if you are cost conscious and opt to use cheaper machines like <a href=https://cloud.google.com/compute/docs/instances/preemptible>GCP preemptible</a> or <a href=https://aws.amazon.com/ec2/spot/>AWS Spot</a> for your workloads. Thanos components <a href=https://thanos.io/tip/thanos/service-discovery.md/#service-discovery>rely on each other</a> and wait for some time to drop an unresponsive peer out of their list. Combine all of these and you get the picture: heavy queries or chaos kills a component; other components still wait for it for a long time before giving up and hence slow down the whole system. This gets especially bold in the fan-out model of Thanos query.</p><h2 id=the-parameters>The Parameters</h2><p>For a quick recap, in a <a href=https://banzaicloud.com/blog/multi-cluster-monitoring/#metric-query-flow>multi-cluster setup</a>, you would be having at least these set of components:</p><ul><li>Thanos Sidecar: to upload TSDB blocks and also respond to queries for fresh data</li><li>Thanos Store: to serve the previously uploaded TSDB blocks</li><li>Thanos Query: To connect these pieces together, do the <a href=https://thanos.io/tip/components/query.md/#deduplication>deduplication</a> you were promised and provide a single endpoint for users</li></ul><p>Tuning these parameters doesn&rsquo;t magically solve the issue; they help to minimize the blast radius of bad events when they happen.</p><h3 id=query-timeout>Query Timeout</h3><p>This is the simplest amount of time a query can be in flight before it is aborted (with error in various layers). This is not set by default, meaning there is no limit, so a heavy query could clog the system for a long time. By setting this in multiple layers (i.e. global query and local query), you can have better control and hard limits against system-wrecking queries.</p><p>This applies to <a href=https://thanos.io/tip/components/query.md/#flags>query</a> component as <code>--query.timeout</code> CLI flag.</p><h3 id=store-response-timeout>Store Response Timeout</h3><p>This can be interpreted as the equivalent of <a href=https://en.wikipedia.org/wiki/Time_to_first_byte>time-to-first-byte</a> in Thanos world. Store components only have one key job to do: find data blocks that match the series select criteria and start streaming their content. This should be fairly fast or at least start fairly fast. By setting this value, we are excluding the instances that take long to send data, which usually means they are dead, dying, or bogged down under another heavy query. Since we have at least two of any store instances (be it sidecar or Thanos store), this is fine. Even if both instances hit this timeout, at least we fail fast and free up the hot path for other (hopefully) non-failing queries.</p><p>This applies to <a href=https://thanos.io/tip/components/query.md/#flags>query</a> component as <code>--store.response-timeout</code> CLI flag.</p><h3 id=grpc-grace-period>gRPC Grace Period</h3><p>This is the period that the gRPC server continues to listen when receives an interrupt. In K8S this happens when a pod is being evicted.</p><p>This applies to <a href=https://thanos.io/tip/components/query.md/#flags>query</a>, <a href=https://thanos.io/tip/components/store.md/#flags>store</a> and <a href=https://thanos.io/tip/components/sidecar.md/#flags>sidecar</a> components as <code>--grpc-grace-period</code> CLI flag.</p><h3 id=series-sample-limit>Series Sample Limit</h3><p>This flag on Thanos store limits the number of samples that a whole query (including subqueries) will touch. The number of samples can increase through cardinality and range of time. So a low cardinality metric for <code>60d</code> might touch a lot of samples, the same as a high cardinality metric over <code>1d</code> might. Why would this matter? because these are often heavy queries that are in dire need of optimization, either by rewriting the query expression or changing the metric from the source.</p><p>This applies to <a href=https://thanos.io/tip/components/store.md/#flags>store</a> component as <code>--store.grpc.series-sample-limit</code> CLI flag.</p><h2 id=the-reasoning>The Reasoning</h2><p>We are addressing two problems:</p><ul><li>Few heavy queries can take all of the system resources, not leaving any room for lighter ones</li><li>Chaotic environment: there will always be some components dying (usual restart or reschedules) across clusters</li></ul><p>We need some solutions for both:</p><ul><li>In lieu of proper prioritization mechanism in this stack, limit the impact of heavy queries</li><li>Improve the responsiveness of the whole system when redundant component unavailability events happen by setting tighter limits and deadlines</li></ul><p>The only levers we have to limit heavy queries are query timeout and sample limit. Query timeout can cap the duration that the said query will keep the system engaged and the sample limit can stop the processing when there are too many samples to return. Tightening these parameters will lead to the heaviest queries failing most —if not all— of the time, but also make the system more resilient as a result. Finding proper values for each required a series of experiments with trial-and-error approach.</p><p>For the chaotic part, the faster we let go of the fallen <del>comrades</del> components in the path, the better. Since there is usually a good amount of redundancy in place and the requests are fanned-out, one failing component shouldn’t drive the latency of the response. In the case of non-failing but slow component (usually stores), if the answer isn’t being streamed within a short time, there is a good chance that it never will be; again, cutting early leads to better performance and at worst results in failing fast. By setting tight grace periods and response times, we minimize the effect of any slow or failing component on global query evaluation.</p><h2 id=the-recommendation>The Recommendation</h2><p>Of course, like many things that you read over the Internet, you shouldn&rsquo;t take them as is; these are just recommendations that worked for me in a specific environment. After reading this, you hopefully understand the parameters and will tune them best to your environment and use case.</p><p>Sidecar</p><ul><li><code>--grpc-grace-period=5s</code></li></ul><p>Store</p><ul><li><code>--store.grpc.series-sample-limit=50000</code></li><li><code>--grpc-grace-period=5s</code></li></ul><p>Local Query</p><ul><li><code>--query.timeout=2m</code></li><li><code>--store.response-timeout=10s</code></li><li><code>--grpc-grace-period=5s</code></li></ul><p>Global Query</p><ul><li><code>--query.timeout=5m</code></li><li><code>--store.response-timeout=10s</code></li><li><code>--grpc-grace-period=5s</code></li></ul></div><div class=tags><ul class=flat><li><a href=/tags/prometheus>prometheus</a></li><li><a href=/tags/thanos>thanos</a></li><li><a href=/tags/monitoring>monitoring</a></li></ul></div></div></div><script async src="https://www.googletagmanager.com/gtag/js?id=G-1ZG9PT72LY"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1ZG9PT72LY")</script><div class="footer wrapper"><nav class=nav><div>2021 All contents under <a href=https://www.gnu.org/licenses/fdl-1.3.html>GNU FDL</a> | <a href=https://github.com/knadh/hugo-ink>Ink</a> theme on <a href=https://gohugo.io>Hugo</a></div></nav></div><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-101616403-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<script>feather.replace()</script></body></html>